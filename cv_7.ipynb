{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GAN for Image Generation"
      ],
      "metadata": {
        "id": "RvW4Hah4o3oQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-JkyHyQn-AI",
        "outputId": "5df02b50-44c7-4bb7-8cdb-97e9536b597f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102845770.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20611824.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 36116578.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6429473.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss D: 0.7750575244426727, Loss G: 3.9478588104248047\n",
            "Epoch 2, Loss D: 0.19816778786480427, Loss G: 3.1270577907562256\n",
            "Epoch 3, Loss D: 0.13926053419709206, Loss G: 3.769843578338623\n",
            "Epoch 4, Loss D: 0.2585000842809677, Loss G: 3.2638847827911377\n",
            "Epoch 5, Loss D: 0.27333416044712067, Loss G: 2.7290661334991455\n",
            "Epoch 6, Loss D: 0.7173106856644154, Loss G: 2.29699969291687\n",
            "Epoch 7, Loss D: 0.6102647483348846, Loss G: 1.6247811317443848\n",
            "Epoch 8, Loss D: 0.7156872749328613, Loss G: 2.2966723442077637\n",
            "Epoch 9, Loss D: 0.5295298397541046, Loss G: 1.735113501548767\n",
            "Epoch 10, Loss D: 0.6513427197933197, Loss G: 1.9565714597702026\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 28*28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1, 1, 28, 28)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x.view(-1, 28*28))\n",
        "\n",
        "# Instantiate models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Load data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "dataloader = DataLoader(datasets.MNIST('.', download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for i, (images, _) in enumerate(dataloader):\n",
        "        # Train Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        labels = torch.ones(images.size(0), 1)\n",
        "        outputs = discriminator(images)\n",
        "        loss_d_real = criterion(outputs, labels)\n",
        "        loss_d_real.backward()\n",
        "\n",
        "        noise = torch.randn(images.size(0), 100)\n",
        "        fake_images = generator(noise)\n",
        "        labels = torch.zeros(images.size(0), 1)\n",
        "        outputs = discriminator(fake_images.detach())\n",
        "        loss_d_fake = criterion(outputs, labels)\n",
        "        loss_d_fake.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        labels = torch.ones(images.size(0), 1)\n",
        "        outputs = discriminator(fake_images)\n",
        "        loss_g = criterion(outputs, labels)\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss D: {loss_d_real.item() + loss_d_fake.item()}, Loss G: {loss_g.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Object Tracking System with DeepSORT"
      ],
      "metadata": {
        "id": "hUKFvqOrqz93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-sort-realtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqd9J67Bs9Qc",
        "outputId": "ee959fa2-fbd0-4849-e0e5-450345850ad0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-sort-realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (4.10.0.84)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/8.4 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-sort-realtime\n",
            "Successfully installed deep-sort-realtime-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load YOLO model (paths should be set correctly)\n",
        "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture('video.mp4')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Object detection\n",
        "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(output_layers)\n",
        "\n",
        "    # Initialize an empty list for detections\n",
        "    detections = []\n",
        "\n",
        "    # Process each output from the YOLO network\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)  # Get the class ID\n",
        "            confidence = scores[class_id]  # Get the confidence\n",
        "\n",
        "            if confidence > 0.5:  # Filter out weak detections\n",
        "                center_x = int(detection[0] * frame.shape[1])\n",
        "                center_y = int(detection[1] * frame.shape[0])\n",
        "                w = int(detection[2] * frame.shape[1])\n",
        "                h = int(detection[3] * frame.shape[0])\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                detections.append((x, y, w, h, confidence))\n",
        "\n",
        "    # Draw bounding boxes around detections\n",
        "    for (x, y, w, h, confidence) in detections:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f'{confidence:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('Frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "fjGcOuBVo6Dm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Captioning System"
      ],
      "metadata": {
        "id": "eGi7TQk-q7om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define Encoder (CNN) and Decoder (RNN) models\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.model(x)\n",
        "        return x.view(x.size(0), -1)  # Shape should be [batch_size, 2048]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size + 2048, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embedding(captions)  # [batch_size, caption_length, embed_size]\n",
        "\n",
        "        # Debugging: Check the shape of features and embeddings\n",
        "        print(f'features shape before unsqueeze: {features.shape}')\n",
        "        print(f'captions shape: {captions.shape}')\n",
        "        print(f'embeddings shape: {embeddings.shape}')\n",
        "\n",
        "        # Ensure features has correct shape\n",
        "        if features.dim() == 2:  # [batch_size, 2048]\n",
        "            features = features.unsqueeze(1)  # [batch_size, 1, 2048]\n",
        "            print(f'features shape after unsqueeze: {features.shape}')\n",
        "\n",
        "        # Repeat features to match the caption length\n",
        "        features = features.repeat(1, captions.size(1), 1)  # [batch_size, caption_length, 2048]\n",
        "        print(f'features shape after repeat: {features.shape}')\n",
        "\n",
        "        # Concatenate features and embeddings\n",
        "        inputs = torch.cat((features, embeddings), dim=2)  # [batch_size, caption_length, 2048 + embed_size]\n",
        "        print(f'inputs shape after concatenation: {inputs.shape}')\n",
        "\n",
        "        outputs, _ = self.lstm(inputs)\n",
        "        return self.fc(outputs)\n",
        "\n",
        "# Load pretrained model and set up data transforms\n",
        "encoder = Encoder()\n",
        "decoder = Decoder(vocab_size=1000, embed_size=256, hidden_size=512)\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load and preprocess an image\n",
        "image = Image.open('apple.jpeg')\n",
        "image = preprocess(image).unsqueeze(0)  # [1, 3, 224, 224]\n",
        "features = encoder(image)  # [1, 2048]\n",
        "\n",
        "# Debugging: Check the shape of the encoded features\n",
        "print(f'Encoded features shape: {features.shape}')\n",
        "\n",
        "# Generate captions\n",
        "captions = torch.LongTensor([[1, 2, 3, 4]])  # Example caption indices with batch size 1\n",
        "outputs = decoder(features, captions)\n",
        "\n",
        "print(f'Final outputs shape: {outputs.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vehcc15q3ho",
        "outputId": "abce9a51-e79f-48c1-ee3c-54e6127b0d24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded features shape: torch.Size([1, 2048])\n",
            "features shape before unsqueeze: torch.Size([1, 2048])\n",
            "captions shape: torch.Size([1, 4])\n",
            "embeddings shape: torch.Size([1, 4, 256])\n",
            "features shape after unsqueeze: torch.Size([1, 1, 2048])\n",
            "features shape after repeat: torch.Size([1, 4, 2048])\n",
            "inputs shape after concatenation: torch.Size([1, 4, 2304])\n",
            "Final outputs shape: torch.Size([1, 4, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JeujkZyrB6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}