{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "def load_cifar10_data():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Load pre-trained model without the top layers\n",
        "def get_base_model(input_shape):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    return base_model\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "def build_model(base_model, num_classes):\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "# Fine-tuning the model\n",
        "def fine_tune_model(model, base_model, learning_rate, num_layers_to_freeze):\n",
        "    # Freeze the base layers\n",
        "    for layer in base_model.layers[:num_layers_to_freeze]:\n",
        "        layer.trainable = False\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    input_size = (32, 32, 3)\n",
        "    num_classes = 10  # CIFAR-10 has 10 classes\n",
        "    learning_rate = 1e-4\n",
        "    num_layers_to_freeze = 50\n",
        "    epochs = 5\n",
        "    batch_size = 64\n",
        "\n",
        "    # Load data\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10_data()\n",
        "\n",
        "    # Load and build model\n",
        "    base_model = get_base_model(input_size)\n",
        "    model = build_model(base_model, num_classes)\n",
        "\n",
        "    # Fine-tune model\n",
        "    model = fine_tune_model(model, base_model, learning_rate, num_layers_to_freeze)\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping, checkpoint]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f'Test loss: {loss}')\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQi3yWVIWkPb",
        "outputId": "a57d4bb7-de10-42af-ad0d-214d46c34283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2325s\u001b[0m 4s/step - accuracy: 0.2449 - loss: 2.2180 - val_accuracy: 0.3366 - val_loss: 1.9191\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2267s\u001b[0m 4s/step - accuracy: 0.4677 - loss: 1.4947 - val_accuracy: 0.2841 - val_loss: 2.1853\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2330s\u001b[0m 4s/step - accuracy: 0.5375 - loss: 1.3054 - val_accuracy: 0.4395 - val_loss: 1.6202\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2290s\u001b[0m 4s/step - accuracy: 0.5799 - loss: 1.1806 - val_accuracy: 0.3801 - val_loss: 1.8447\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2302s\u001b[0m 4s/step - accuracy: 0.6013 - loss: 1.1229 - val_accuracy: 0.3072 - val_loss: 2.6650\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 132ms/step - accuracy: 0.4471 - loss: 1.6088\n",
            "Test loss: 1.6153405904769897\n",
            "Test accuracy: 0.4404999911785126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gquso-1oWsYk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}